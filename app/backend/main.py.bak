from fastapi import FastAPI, UploadFile, HTTPException
from pydantic import BaseModel
from typing import Optional
import yt_dlp
import whisper
import requests
import tempfile
import os
from pathlib import Path
import logging
from dotenv import load_dotenv
from fastapi.middleware.cors import CORSMiddleware
from langchain_community.embeddings import HuggingFaceEmbeddings

from services.text_splitter import TextSplitter
from services.user_manager import UserIdentifier
from services.pod_fetcher import PodFetcher
from db.qdrant_db import QdrantDB

# TEMPORARY FOR TESTING PURPOSES
import time
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
username = "AverageJoe"
collection_name = UserIdentifier.get_collection_name(username)
vector_db = QdrantDB(collection_name=collection_name, embedding_model=embedding_model)
text_splitter = TextSplitter(chunk_size=500, chunk_overlap=50)


# Load environment variables
load_dotenv()

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load Whisper model
try:
    model = whisper.load_model("base")
    logger.info("Whisper model loaded successfully")
except Exception as e:
    logger.error(f"Error loading Whisper model: {str(e)}")
    raise

class TranscriptionRequest(BaseModel):
    youtube_url: Optional[str] = None
    podcast_url: Optional[str] = None

class URLRequest(BaseModel):
    url: str

class ChatRequest(BaseModel):
    message: str
    context: Optional[str] = None  # For passing transcription context

def download_youtube_audio(url: str) -> str:
    """Download audio from YouTube video and save to temporary file"""
    logger.info(f"Starting download of YouTube video: {url}")
    ydl_opts = {
        'format': 'bestaudio/best',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
        'outtmpl': '%(id)s.%(ext)s'
    }
    
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=True)
            audio_file = f"{info['id']}.mp3"
            logger.info(f"Successfully downloaded audio: {audio_file}")
            return audio_file
    except Exception as e:
        logger.error(f"Error downloading YouTube video: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Failed to download YouTube video: {str(e)}")

def transcribe_audio(audio_path: str) -> str:
    """Transcribe audio file using Whisper model"""
    logger.info(f"Starting transcription of audio file: {audio_path}")
    try:
        result = model.transcribe(audio_path)
        logger.info("Transcription completed successfully")
        return result["text"]
    except Exception as e:
        logger.error(f"Error transcribing audio: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Failed to transcribe audio: {str(e)}")

def generate_summary_ollama(text: str) -> str:
    """Generate summary using Ollama local model"""
    logger.info("Starting summary generation with Ollama")
    try:
        # Truncate very long texts to avoid memory issues
        max_length = 8000  # Adjust based on your needs
        if len(text) > max_length:
            text = text[:max_length] + "... [truncated for processing]"
            logger.info(f"Text truncated to {max_length} characters for processing")
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2:3b",
                "prompt": f"Please provide a concise summary of the following text, highlighting the key learnings and main points:\n\n{text}",
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_ctx": 4096  # Context window size
                }
            },
            timeout=300  # 5 minute timeout for summary generation
        )
        response.raise_for_status()
        result = response.json()
        logger.info("Summary generated successfully with Ollama")
        return result["response"]
    except requests.exceptions.Timeout:
        logger.error("Timeout while generating summary with Ollama")
        raise HTTPException(status_code=408, detail="Summary generation timed out. The content might be too long.")
    except requests.exceptions.ConnectionError:
        logger.error("Cannot connect to Ollama. Make sure Ollama is running.")
        raise HTTPException(status_code=503, detail="Cannot connect to Ollama. Please make sure Ollama is running on localhost:11434")
    except Exception as e:
        logger.error(f"Error generating summary with Ollama: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to generate summary: {str(e)}")

def chat_with_ollama(message: str, context: str = None) -> str:
    """Chat with Ollama model, optionally with context from transcription"""
    logger.info("Starting chat with Ollama")
    try:
        if context:
            # Limit context size to avoid token limits
            max_context = 3000
            if len(context) > max_context:
                context = context[:max_context] + "... [truncated]"
            prompt = f"Based on this content: {context}\n\nUser question: {message}\n\nPlease provide a helpful response based on the content above."
        else:
            prompt = message
            
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2:3b",
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_ctx": 4096
                }
            },
            timeout=120  # 2 minute timeout for chat
        )
        response.raise_for_status()
        result = response.json()
        logger.info("Chat response generated successfully")
        return result["response"]
    except requests.exceptions.Timeout:
        logger.error("Timeout while generating chat response")
        raise HTTPException(status_code=408, detail="Response generation timed out. Please try a simpler question.")
    except requests.exceptions.ConnectionError:
        logger.error("Cannot connect to Ollama for chat")
        raise HTTPException(status_code=503, detail="Cannot connect to Ollama. Please make sure Ollama is running.")
    except Exception as e:
        logger.error(f"Error in chat with Ollama: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to generate chat response: {str(e)}")

def store_transcript_in_vector_db(transcript: str, source: str, url: str, username: str = "AverageJoe"):
    """
    Splits a transcript into chunks and inserts them into the Qdrant vector DB
    with metadata for retrieval.
    """
    logger.info("Splitting transcript into chunks for vector DB storage...")
    chunks = text_splitter.split_text(transcript)

    texts = []
    metadatas = []

    for i, chunk in enumerate(chunks):
        texts.append(chunk)
        metadatas.append({
            "source": source,
            "url": url,
            "chunk_index": i,
            "username": username
        })

    if texts:
        vector_db.vectorstore.add_texts(texts=texts, metadatas=metadatas)
        logger.info(f"Inserted {len(texts)} chunks into vector DB for {source} at {url}")
    else:
        logger.warning("No chunks were created from transcript; skipping vector DB insert.")


@app.post("/process-youtube")
async def process_youtube(request: TranscriptionRequest):
    """Process YouTube video: download audio, transcribe, and summarize"""
    if not request.youtube_url:
        raise HTTPException(status_code=400, detail="YouTube URL is required")
    
    logger.info(f"Processing YouTube URL: {request.youtube_url}")
    audio_file = None
    
    try:
        # Download audio from YouTube
        audio_file = download_youtube_audio(request.youtube_url)
        
        # Transcribe audio
        transcription = transcribe_audio(audio_file)

        # Store transcript in vector database
        store_transcript_in_vector_db(transcription, source="youtube", url=request.youtube_url, username=username)
        logger.info("Transcript stored in vector database")
        
        # Generate summary using Ollama
        summary = generate_summary_ollama(transcription)
        
        # Clean up temporary audio file
        try:
            os.remove(audio_file)
            logger.info(f"Cleaned up temporary file: {audio_file}")
        except Exception as e:
            logger.warning(f"Failed to clean up temporary file: {str(e)}")
        
        return {
            "transcription": transcription,
            "summary": summary
        }
    except Exception as e:
        logger.error(f"Error in process_youtube: {str(e)}")
        if audio_file and os.path.exists(audio_file):
            try:
                os.remove(audio_file)
            except:
                pass
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-audio")
async def process_audio(file: UploadFile):
    """Process uploaded audio file: transcribe and summarize"""
    if not file:
        raise HTTPException(status_code=400, detail="Audio file is required")
    
    logger.info(f"Processing uploaded audio file: {file.filename}")
    temp_path = None
    
    try:
        # Save uploaded file to temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name
            logger.info(f"Saved temporary file: {temp_path}")
        
        # Transcribe audio
        transcription = transcribe_audio(temp_path)
        
        # Generate summary using Ollama
        summary = generate_summary_ollama(transcription)
        
        # Clean up temporary file
        try:
            os.remove(temp_path)
            logger.info(f"Cleaned up temporary file: {temp_path}")
        except Exception as e:
            logger.warning(f"Failed to clean up temporary file: {str(e)}")
        
        return {
            "transcription": transcription,
            "summary": summary
        }
    except Exception as e:
        logger.error(f"Error in process_audio: {str(e)}")
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/process-podcast")
async def process_podcast(request: TranscriptionRequest):
    """Process podcast: download episode (audio or transcript), transcribe if needed, and summarize"""
    if not request.podcast_url:
        raise HTTPException(status_code=400, detail="Podcast URL is required")
    
    logger.info(f"Processing podcast URL: {request.podcast_url}")
    fetcher = PodFetcher()
    file_path = None

    try:
        # Download podcast file
        info = fetcher.fetch(request.podcast_url)
        file_path = info["filepath"]
        logger.info(f"Downloaded {info['download_type']} to {file_path}")

        # Handle transcription
        if info["download_type"] == "transcript":
            with open(file_path, "r", encoding="utf-8") as f:
                transcription = f.read()
            logger.info("Loaded transcript from file")
        else:
            transcription = transcribe_audio(file_path)
            logger.info("Audio file transcribed successfully")

        # Store transcript in vector database
        store_transcript_in_vector_db(transcription, source="podcast", url=request.podcast_url, username=username)
        logger.info("Transcript stored in vector database")

        # Generate summary
        summary = generate_summary_ollama(transcription)
        logger.info("Summary generated successfully")

        # Clean up
        try:
            os.remove(file_path)
            logger.info(f"Cleaned up temporary file: {file_path}")
        except Exception as e:
            logger.warning(f"Failed to clean up file: {str(e)}")

        return {
            "transcription": transcription,
            "summary": summary
        }

    except Exception as e:
        logger.error(f"Error in process_podcast: {str(e)}")
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=str(e))

    except Exception as e:
        logger.error(f"Error in process_podcast: {str(e)}")
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/process-url")
async def process_url(request: URLRequest):
    url = request.url.strip()

    if "youtube.com" in url or "youtu.be" in url:
        logger.info("Detected YouTube URL")
        transcription_request = TranscriptionRequest(youtube_url=url)
        return await process_youtube(transcription_request)
    
    elif "podcasts.apple.com" in url:
        logger.info("Detected Apple Podcast URL")
        transcription_request = TranscriptionRequest(podcast_url=url)
        return await process_podcast(transcription_request)
    
    else:
        logger.warning("Unrecognized URL format")
        raise HTTPException(status_code=400, detail="Unsupported URL type")


@app.post("/chat")
async def chat(request: ChatRequest):
    """Chat endpoint for conversing about transcribed content or general questions"""
    try:
        # IN PROGRESS: Integrate vector DB search and chat with Ollama. Currently passing the entire transcript to Ollama which won't work if we have long transcripts and multiple podcasts
        '''
        start = time.time()
        vector_db_results = vector_db.search(request.message, k=5)
        logger.info(f"Vector DB search completed in {time.time() - start:.2f} seconds")
        start = time.time()
        response = chat_with_ollama(request.message, vector_db_results) #do we need to format the context?  ie. "chunk 1: {chunk_content}. \n chunk 2: {chunk_content} \n" etc.
                                                                        #Currently wasting a lot of tokens by passing the whole vector DB result
        logger.info(f"Chat response generated in {time.time() - start:.2f} seconds")
        '''
        response = chat_with_ollama(request.message, request.context)
        return {"response": response}
    except Exception as e:
        logger.error(f"Error in chat endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint that also verifies Ollama connection"""
    try:
        # Check if Ollama is running
        response = requests.get("http://localhost:11434/api/version", timeout=5)
        ollama_status = "connected" if response.status_code == 200 else "disconnected"
    except:
        ollama_status = "disconnected"
    
    return {
        "status": "healthy",
        "ollama": ollama_status,
        "whisper": "loaded"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)