"""
qdrant_db.py

This module defines the QdrantDB class for interacting with a Qdrant vector database
running locally (via Docker at http://localhost:6333).

The QdrantDB class:
  - Initializes a Qdrant client.
  - Ensures the target collection exists (creating it if it doesnâ€™t). 1 collection per USER.
  - Instantiates a LangChain Qdrant vector store that uses the provided embedding model.
  - Provides methods to add texts (document chunks) to the collection.
  - Provides a method to perform similarity searches on the collection.

Assumptions:
  - Qdrant is running locally.
  - An embedding model (e.g., HuggingFaceEmbeddings from langchain_community) is provided.
"""
import os
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from langchain_community.vectorstores import Qdrant 
from langchain_core.embeddings import Embeddings  

# Load environment variables.
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "../.env"))

class QdrantDB:
    def __init__(self, collection_name: str, embedding_model: Embeddings, vector_size: int = 384, client_url: str = "http://localhost:6333"):
        """
        Initialize the QdrantDB instance.
        """
        self.collection_name = collection_name
        self.embedding_model = embedding_model
        self.vector_size = vector_size
        self.client_url = client_url

        self.client = QdrantClient(url=self.client_url)

        self._ensure_collection_exists()

        self.vectorstore = Qdrant(
            client=self.client,
            collection_name=self.collection_name,
            embeddings=self.embedding_model 
        )

    def _ensure_collection_exists(self) -> None:
        """
        Ensure that the collection exists in Qdrant, or create it.
        """
        try:
            collection = self.client.get_collection(collection_name=self.collection_name)
            if collection:
                print(f"Collection '{self.collection_name}' already exists.")
            else:
                raise Exception("Collection not found.")
        except Exception:
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)
            )
            print(f"Created collection: {self.collection_name}")

    def add_texts(self, texts: list[str], metadata: list[dict] = None) -> None:
        """
        Add a list of text chunks to the Qdrant collection with optional metadata.
        
        Args:
            texts (list): List of text strings to add to the collection.
            metadata (list): Optional list of metadata dictionaries corresponding to each text.
        """
        if metadata and len(metadata) != len(texts):
            raise ValueError("Length of metadata must match length of texts")

        self.vectorstore.add_texts(texts=texts, metadatas=metadata)
        print(f"Successfully added {len(texts)} texts to collection: {self.collection_name}")


    def search(self, query: str, k: int = 5) -> list:
        """
        Perform similarity search against the collection.
        """
        return self.vectorstore.similarity_search(query, k=k)


# -------------------- Testing Block --------------------

if __name__ == "__main__":
    import sys
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../")))

    from langchain_community.embeddings import HuggingFaceEmbeddings  
    from app.backend.services.text_splitter import TextSplitter
    from app.backend.services.user_manager import UserIdentifier

    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    username = "AverageJoe"
    collection_name = UserIdentifier.get_collection_name(username)

    db = QdrantDB(collection_name=collection_name, embedding_model=embedding_model)

    transcript_path = os.path.join(os.path.dirname(__file__), "../../../test_data/transcript_dwarkesh-ama-ft-sholto-trenton.txt")
    splitter = TextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_file(transcript_path)

    db.add_texts(chunks)

    query = "what do they say about the future of tech careers in relation to AI?"
    results = db.search(query, k=5)

    for i, doc in enumerate(results):
        print(f"\nCHUNK {i+1}:\n{doc.page_content}")
